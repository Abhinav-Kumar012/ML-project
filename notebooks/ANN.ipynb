{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import re\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder,OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# TensorFlow and Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv('../input/credit-dset/clean_trained.csv')\n",
    "df_test = pd.read_csv('../input/credit-dset/test_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode_cols = ['Month','Profession']\n",
    "for i in one_hot_encode_cols:\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    encoded_train = ohe.fit_transform(df_train[[i]])\n",
    "    encoded_test = ohe.transform(df_test[[i]])\n",
    "    encoded_cols = [f\"{i}_{j}\" for j in ohe.categories_[0]]\n",
    "    df_train_encoded = pd.DataFrame(encoded_train, columns=encoded_cols)\n",
    "    df_test_encoded = pd.DataFrame(encoded_test, columns=encoded_cols)\n",
    "    df_train = pd.concat([df_train, df_train_encoded], axis=1).drop(columns=[i])\n",
    "    df_test = pd.concat([df_test, df_test_encoded], axis=1).drop(columns=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_mix_dict = {\"Bad\" : 0,\"Standard\" : 1,\"Good\" : 2}\n",
    "df_train['Credit_Mix'] = df_train['Credit_Mix'].map(cred_mix_dict)\n",
    "df_test['Credit_Mix'] = df_test['Credit_Mix'].map(cred_mix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['pay_type','val_pay']] = df_train['Payment_Behaviour'].str.split(pat = '_spent_',n=1,expand = True)\n",
    "df_test[['pay_type','val_pay']] = df_test['Payment_Behaviour'].str.split(pat = '_spent_',n=1,expand = True)\n",
    "df_train = df_train.drop(['Payment_Behaviour'],axis=1)\n",
    "df_test = df_test.drop(['Payment_Behaviour'],axis=1)\n",
    "pay_type_dict = {'Low' : 0, 'High' : 1}\n",
    "val_pay_dict = {'Small_value_payments' : 0,'Medium_value_payments' : 1,'Large_value_payments' : 2}\n",
    "df_train['pay_type'] = df_train['pay_type'].map(pay_type_dict)\n",
    "df_test['pay_type'] = df_test['pay_type'].map(pay_type_dict)\n",
    "df_train['val_pay'] = df_train['val_pay'].map(val_pay_dict)\n",
    "df_test['val_pay'] = df_test['val_pay'].map(val_pay_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_no_dict = {'Yes' : 1,'No' : 0}\n",
    "df_train['Payment_of_Min_Amount'] = df_train['Payment_of_Min_Amount'].map(yes_no_dict)\n",
    "df_test['Payment_of_Min_Amount'] = df_test['Payment_of_Min_Amount'].map(yes_no_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final = OrdinalEncoder(categories=[['Poor', 'Standard', 'Good']])\n",
    "df_train['Credit_Score'] = encoder_final.fit_transform(df_train[['Credit_Score']])\n",
    "df_train['Credit_Score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['Credit_Score','Number'],axis=1)\n",
    "Y = df_train['Credit_Score']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df_test=df_test.drop(['Number'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reshaped = X_scaled.reshape(-1, X_scaled.shape[1], 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.4614 - loss: 1.8169 - val_accuracy: 0.5848 - val_loss: 1.5427\n",
      "Epoch 2/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.5685 - loss: 1.5271 - val_accuracy: 0.5977 - val_loss: 1.4225\n",
      "Epoch 3/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.6081 - loss: 1.4334 - val_accuracy: 0.6212 - val_loss: 1.3368\n",
      "Epoch 4/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.6263 - loss: 1.3722 - val_accuracy: 0.6403 - val_loss: 1.2898\n",
      "Epoch 5/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.6360 - loss: 1.3320 - val_accuracy: 0.6533 - val_loss: 1.2603\n",
      "Epoch 6/500\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - accuracy: 0.6400 - loss: 1.3068 - val_accuracy: 0.6591 - val_loss: 1.2392\n",
      "Epoch 7/500\n",
      "\u001b[1m22/63\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.6409 - loss: 1.2856"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(256, activation='relu',),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.35),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=l1(1e-4)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003),  # Reduced learning rate\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping with patience and best weight restoration\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               patience=35,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# Model training\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=1024,\n",
    "    epochs=500,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Model architecture\n",
    "final_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(256, activation='relu',),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.35),\n",
    "\n",
    "    Dense(512, activation='relu', kernel_regularizer=l1(1e-4)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Model training\n",
    "history = model.fit(x=X_train_scaled,\n",
    "                    y=y_train,\n",
    "                    batch_size=1024,\n",
    "                    epochs=260,\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
