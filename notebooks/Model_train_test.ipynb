{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries which are important for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, which are cleaned by pre-processing the original datasets by various methods like imputing values with null values and other methods. Label Encode the columns having object datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('input/credit-dset/clean_trained.csv')\n",
    "test_df = pd.read_csv('input/credit-dset/test_cleaned_final.csv')\n",
    "\n",
    "# Encode the target variable in the training data\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Credit_Score'] = label_encoder.fit_transform(train_df['Credit_Score'])\n",
    "\n",
    "# Columns to label encode\n",
    "label_encode_cols = ['Month', 'Profession', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data into training and testing and also fit transform the label encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply label encoding to each specified column\n",
    "for col in label_encode_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = train_df.drop(columns=['Credit_Score', 'Number'])  # Exclude target and unnecessary columns\n",
    "y = train_df['Credit_Score']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Random Forest model and XG Boost Model with various parameters for doing hyperparameter tuning on these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define parameter grids for RandomizedSearchCV\n",
    "# Define expanded parameter grid for RandomForest\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__max_depth': [10, 20, 30, 40, 50, None],  # None to allow trees to expand until all leaves are pure\n",
    "    'classifier__min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4, 8, 10],\n",
    "\n",
    "}\n",
    "# Define expanded parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300,500],\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, 9, 11, 13],\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Randomized Search CV to try on 12 iterations with various possibilities and use 5-fold cross validation to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define pipelines for both models without preprocessing\n",
    "rf_pipeline = Pipeline(steps=[('classifier', RandomForestClassifier(random_state=42))])\n",
    "xgb_pipeline = Pipeline(steps=[('classifier', XGBClassifier(eval_metric='mlogloss', random_state=42))])\n",
    "\n",
    "# Perform RandomizedSearchCV for RandomForest\n",
    "rf_search = RandomizedSearchCV(rf_pipeline, rf_param_grid, n_iter=12, scoring='accuracy', cv=5, random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "best_rf_pipeline = rf_search.best_estimator_\n",
    "rf_best_accuracy = accuracy_score(y_val, best_rf_pipeline.predict(X_val))\n",
    "print(f\"Tuned RandomForest Validation Accuracy: {rf_best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do XGB model with same number of iterations and cross validation and also print the accuracy for both xgb and rf. Then compare the accuracy of both the models and take the best model. After that retrain the best model for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform RandomizedSearchCV for XGBoost\n",
    "xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_param_grid, n_iter=12, scoring='accuracy', cv=5, random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train, y_train)\n",
    "best_xgb_pipeline = xgb_search.best_estimator_\n",
    "xgb_best_accuracy = accuracy_score(y_val, best_xgb_pipeline.predict(X_val))\n",
    "print(f\"Tuned XGBoost Validation Accuracy: {xgb_best_accuracy:.4f}\")\n",
    "\n",
    "# Choose the best model\n",
    "best_model = best_xgb_pipeline if xgb_best_accuracy > rf_best_accuracy else best_rf_pipeline\n",
    "\n",
    "# After selecting the best model, retrain on the entire training dataset\n",
    "best_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict the values for rest of test data and get the predictions in the submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the submission with the fully trained model predictions\n",
    "\n",
    "# Prepare test data predictions for submission\n",
    "test_features = test_df.drop(columns=['ID', 'Number'], errors='ignore')  # Exclude unnecessary columns\n",
    "test_preds = best_model.predict(test_features)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Predicted': label_encoder.inverse_transform(test_preds)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
